{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "\n",
    "The goal of this notebook is get you familiar with the ideas behind linear regression and how to implement this method in python. \n",
    "\n",
    "### What is linear regression?\n",
    "\n",
    "How well can you fit a line to data points by eye? https://fditraglia.shinyapps.io/regression/\n",
    "\n",
    "When we have a point cloud, or here measurements of two variables, we might notice the variables seem to lie near a line. Linear regression helps us find the line that describes our points the 'best'.\n",
    "\n",
    "Today we'll use [statsmodels](https://www.statsmodels.org/stable/index.html) to perform our linear regression. Another popular packages is [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), which we'll use in part 2.\n",
    "\n",
    "Thanks to Sara Casella for providing the data and much of the code for this tutorial :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's look at the presumed relationship between SAT scores and college gpa. Does scoring higher on the SAT really suggest the student will perform better in college? If I worked to gain 10 more points on the SAT, how much should I expect my college GPA to increase?\n",
    "\n",
    "We'll read in a large data frame, but we will begin with focusing on the following variables:\n",
    " \n",
    "**colgpa**:   College GPA on a four point scale\n",
    "\n",
    "**sat**:      Combined SAT score (verbal plus math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/college_gpa.csv' does not exist: b'data/college_gpa.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-05a93cdaf093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcollegeDataAll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/college_gpa.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcollegeData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollegeDataAll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'colgpa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcollegeData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/college_gpa.csv' does not exist: b'data/college_gpa.csv'"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "\n",
    "collegeDataAll = pd.read_csv('data/college_gpa.csv')\n",
    "collegeData = collegeDataAll[['sat','colgpa']]\n",
    "collegeData.head()\n",
    "\n",
    "# collegeData.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "\n",
    "# We could also make numpy arrays from our pandas dataframe so that our plotting would look just like last week.\n",
    "# When using a pandas dataframe with seaborn, we input the whole dataframe and then assign the x and y variables\n",
    "# using the column names.\n",
    "sns.scatterplot(data = collegeData, x='sat', y='colgpa', alpha = 0.2)  # Why is the alpha = 0.2 helpful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, looks like a big blob. Though perhaps there is some relationship between SAT score and gpa..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing linear regression.\n",
    "\n",
    "Let's hypothesize that our data should lie along a line. \n",
    "\n",
    "Recall the equation of a line is\n",
    "\\begin{equation}\n",
    "y = a + bx\n",
    "\\end{equation}\n",
    "\n",
    "where $a$ is the y-intercept and $b$ is the slope of the line.\n",
    "\n",
    "Let's replace x and y with our variables. This is the line we are trying to find.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{college GPA} = a + b \\text{SAT score}\n",
    "\\end{equation}\n",
    "\n",
    "As we saw before, we could choose many values for $a$ or $b$ that result in a decent-fitting line. But which one is the _best_ fitting line? We say the line of best fit is the line that minimimzes the sum of squared vertical deviations, also called the sum of squared residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression usings statsmodels\n",
    "\n",
    "# The formula argument tells statsmodels what we think the linear relationship is. If we write down our equtaion (above)\n",
    "# as y = a + bx, then we enter y ~ x into the function. The function will add the y-intercept term, a, and a coefficient,\n",
    "# b, to any term on the right hand side.\n",
    "model = smf.ols(formula = 'colgpa ~ sat', data = collegeData)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That table can be overwhelming. Let's extract just what we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "\n",
    "print('a=', results.params[0])\n",
    "print('b=', results.params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAY we did our first linear regression!\n",
    "\n",
    "Pretty simple huh? Let's discuss what we found so far.\n",
    "\n",
    "1. The slope $b$ is positive, which supports the idea that higher SAT scores correspond to higher GPAs.\n",
    "2. 100 points more in the SAT are associated to $100*0.0019 = 0.19$ points more in the college GPA.\n",
    "\n",
    "How good was our estimate of these parameters?\n",
    "\n",
    "## Uncertainty\n",
    "\n",
    "Just like when we estimated the mean of a population from a sample, above we have a sample and we are estimating the slope $b$ of the _real_ regression line (more on this later). With any estimate from a sample, we expect some error. Just like with the sample mean, we can calculate the _standard error of the slope_ which quantifies the uncertainty in our estimate. Low standard error indicates a more certain estimate. \n",
    "\n",
    "Similarly, we can also calcuate confidence intervals for our estimates of slope and y-intercept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "print(\"Standard error of b=\", results.bse[0])\n",
    "print(\"Confidence interval for b=[0.002, 0.002]\")\n",
    "print(\"Confidence interval for a=[0.526, 0.800]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this linear description of the relationship, we can use the slope and intercept estimates to make predictions for _new_ data. \n",
    "\n",
    "**Prediction**: if your SAT score was 1300, your college GPA will be around to $0.663 + 1300*0.0019 = 3.13$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted values: ', results.predict())\n",
    "\n",
    "sns.scatterplot(data = collegeData, x='sat', y='colgpa', alpha = 0.2, label = \"data\")\n",
    "sns.lineplot(collegeData[\"sat\"], results.predict(), color=\"navy\", label = \"Line of best fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify our plotting, we can also use seaborn regplot\n",
    "\n",
    "sns.regplot(data = collegeData, x='sat', y='colgpa', scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Let's do another linear regression, with two variables now. Let's add high school performance to the list of variables that might predict college GPA.\n",
    " \n",
    "The variables we are interested in now are:\n",
    " \n",
    "**colgpa**:   College GPA on a four point scale\n",
    "\n",
    "**sat**:      Combined SAT score (verbal plus math)\n",
    "\n",
    "**hsperc**   Percentile in HS graduating class (hsperc = 5 top 5%)\n",
    "\n",
    "and our statistical model is \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{college GPA} = a + b_1 \\ \\text{SAT score} +  b_2 \\ \\text{HS percentile} \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first have a look at what hsperc looks like\n",
    "sns.distplot(collegeDataAll['hsperc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the regression\n",
    "\n",
    "model = smf.ols(formula = 'colgpa ~  sat + hsperc', data = collegeDataAll)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- What is the coefficient on HS percentile?\n",
    "\n",
    "- What happened to the coefficient on SAT score once we introduced HS percentile? Why do you think that is?\n",
    "\n",
    "**Research** : how many variables and which variables should I include in a regression? This is not an easy question at all! :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 2: Standing long jumps\n",
    "\n",
    "Do taller people jump further in a standing long jump in general? Let's examine the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "heights = np.array([69. , 63. , 71. , 66. , 71. , 61. , 67. , 65. , 64. , 66. , 74. ,\n",
    "       68. , 67. , 68. , 67. , 73. , 62. , 75. , 68. , 66. , 64. , 67. ,\n",
    "       69. , 67. , 69. , 75. , 67. , 60. , 72. , 63. , 75.5, 68. , 66. ,\n",
    "       61. , 70. , 62. , 62. , 63. , 65. , 68. , 63.5, 69. , 52. , 69. ,\n",
    "       71.5, 73. , 70. , 65. , 64. , 64. ])\n",
    "jump1 = np.array([ 50.2, 72, 84, 80, 56.5, 43, 78, 75, 39, 35, 87, 72, 92, 97, 72.5,\n",
    "       100, 51.5, 85, 63, 62, 51, 33, 89, 79, 75.5, 80, 86, 66, 86, 58,\n",
    "       102, 63, 44, 43, 92, 54.5, 48, 25, 43, 68, 54.5, 100, 88,\n",
    "       100, 65, 87, 54.25, 41, 34, 71.3])\n",
    "jump2 = np.array([57.4, 72.5, 78, 81, 60.5, 45, 75, 74, 40, 34, 90, 75, 94, 101, 69,\n",
    "       105, 46, 91, 70, 60.5, 53, 35, 87, 78, 82, 83, 88, 75, 89, 59, 103,\n",
    "       65, 62, 48, 95, 62, 40, 35, 43, 70, 54.5, 113, 84, 107,\n",
    "       70, 90, 55.75, 37, 32, 74.6])\n",
    "avg_jump = np.mean([jump1, jump2], axis=0)\n",
    "\n",
    "# Let's put the data into a pandas dataframe to match what we did before\n",
    "jump_df = pd.DataFrame({\"height\": heights, \"jump1\": jump1, \"jump2\": jump2, \"avg_jump\": avg_jump})\n",
    "jump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a scatterplot of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a linear regression to test the relationship between average jump distance and height\n",
    "\n",
    "model2 = smf.ols(formula = 'avg_jump ~ height', data = jump_df)\n",
    "results2 = model2.fit()\n",
    "\n",
    "sns.regplot(heights,avg_jump)\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\")\n",
    "plt.title(f\"Relation between height and jump distance\\n y={results2.params[0]} + {results2.params[1]}x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of outliers\n",
    "\n",
    "Let's say now we also included Byron Jones' jump of 146.75 inches. Byron is 73 inches tall. How does that change our regression line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights_byron = np.append(heights,73)\n",
    "avg_jump_byron = np.append(avg_jump,146.73)\n",
    "jump_df2 = pd.DataFrame({\"height\": heights_byron, \"avg_jump\": avg_jump_byron})\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.regplot(heights,avg_jump)\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\")\n",
    "plt.title(f\"Height v jump distance\\n y={results2.params[0]} + {results2.params[1]}x\")\n",
    "\n",
    "\n",
    "\n",
    "# Redo linear regression\n",
    "model3 = smf.ols(formula = 'avg_jump ~ height', data = jump_df2)\n",
    "results3 = model3.fit()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "sns.regplot(heights_byron,avg_jump_byron)\n",
    "sns.scatterplot([73], [146.73], color =\"red\", label = \"Byron Jones\")\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\")\n",
    "plt.title(f\"Height v jump distance including Byron\\n y={results3.params[0]} + {results3.params[1]}x\")\n",
    "\n",
    "\n",
    "plt.subplots_adjust(bottom=None, right=1, top=None, wspace=0.5, hspace = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers** can greatly alter the estimates and even direction of your regression line. If you have outliers, first examine why they might be an outlier (is there some underlying biology that greatly changes their behavior?), and *consider* removing. Perhaps the most ethical option is to show the fit with the outlier, and then show the fit after removing the outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Confidence bands and confidence in predictions\n",
    "\n",
    "What is that blue shaded area around the regression line plotted by regplot that we have expertly ignored until now? This is the 95% confidence band. \n",
    "\n",
    "The **confidence band** measures the precision of the predicted mean Y for each value of X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The confidence band is shown in red in the following plot\n",
    "\n",
    "sns.regplot(heights, avg_jump, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\")\n",
    "plt.title(\"Confidence bands\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the same as predicting an individual new y-value? \n",
    "\n",
    "Let's say I make a standing long jump. How confident can you be in the prediction of *my jump distance*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drB_height = 67\n",
    "drB_jump = 70\n",
    "\n",
    "sns.regplot(heights, avg_jump)\n",
    "sns.scatterplot([drB_height], [drB_jump], color = \"orange\", s=100)\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confidence band only describes precision in predicting the *mean* Y value for the given X value. Individuals have variation, so in order to capture the precision in estimating a new individual data point, we need the prediction intervals.\n",
    "\n",
    "The **prediction interval** meaures the precision of the predicted single Y-values for each X. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the prediction intervals\n",
    "\n",
    "a_height = results2.params[0]\n",
    "b_height = results2.params[1]\n",
    "\n",
    "# Calculate the predicted average jump values by hand. Usually predicted values mathematically are written with a 'hat'.\n",
    "avg_jump_hats = a_height + b_height*heights\n",
    "\n",
    "# Calculate the sum of squared residuals by hand\n",
    "MS_resid_jump = (np.sum((avg_jump - avg_jump_hats)**2))/(heights.shape[0] -2)\n",
    "\n",
    "# Calculate the standard error\n",
    "SE_predict_heights = np.sqrt(MS_resid_jump * ((1+1/heights.shape[0]1+((heights-np.mean(heights))**2)/(np.sum((heights-np.mean(heights))**2)))))\n",
    "t = stats.t.ppf(0.975, heights.shape[0]-2)\n",
    "\n",
    "# Calculate the prediction intervals\n",
    "prediction_upper = avg_jump_hats - t*SE_predict_heights\n",
    "prediction_lower = avg_jump_hats + t*SE_predict_heights\n",
    "\n",
    "sns.regplot(heights,avg_jump)\n",
    "sns.lineplot(heights, prediction_upper, color = \"gray\")\n",
    "sns.lineplot(heights, prediction_lower, color=\"gray\", label=\"Prediction intervals\")\n",
    "sns.scatterplot([drB_height], [drB_jump], color = \"orange\", s=100)\n",
    "plt.xlabel(\"Height (in)\")\n",
    "plt.ylabel(\"Avg Jump (in)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
