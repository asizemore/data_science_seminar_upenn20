{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression - extra exercises\n",
    "\n",
    "The following exercises will go through some extra topics to help improve your understanding and intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Important for regression, curve fitting, smoothing\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Set size of font for matplotplib\n",
    "plt.rc('font', size=12) \n",
    "\n",
    "print(\"Done importing packages :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## RNA and Protein relationship\n",
    "The central dogma tells us that DNA is transcribed to RNA which is translated into a protein. Proteins are difficult to observe in the cell, but RNA is quite 'easy'. So often we record the amount of RNA for gene X and assume that we can use the RNA count to estimate the protein count. Is this a good assumption?\n",
    "\n",
    "If yes, then we should see a highly correlated, very linear relationship between the RNA and protein counts.\n",
    "\n",
    "Below we'll investigate using 30 randomly sampled genes from the data in [Schwanh√§usser et al., 2011](https://www-nature-com.proxy.library.upenn.edu/articles/nature10098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNA molecules/cell in the first column, matched protein molecules/cell in the second column.\n",
    "rna_protein=np.array([[4.81300000e+01, 7.59187000e+04],\n",
    "       [1.04500000e+01, 4.71734900e+04],\n",
    "       [5.91000000e+00, 2.81650200e+04],\n",
    "       [3.31000000e+00, 8.01429000e+03],\n",
    "       [3.08400000e+01, 5.10501640e+05],\n",
    "       [2.29600000e+01, 8.32250900e+04],\n",
    "       [3.63000000e+00, 9.47810000e+02],\n",
    "       [1.55800000e+01, 2.89878500e+04],\n",
    "       [8.50000000e+00, 2.68726130e+05],\n",
    "       [2.00000000e+01, 3.03753130e+05],\n",
    "       [2.48000000e+00, 2.36163800e+04],\n",
    "       [3.21800000e+01, 1.41199720e+05],\n",
    "       [2.38030000e+02, 2.67488198e+06],\n",
    "       [8.90700000e+01, 1.97755860e+05],\n",
    "       [1.76700000e+01, 3.08072700e+04],\n",
    "       [1.19570000e+02, 1.35270000e+02],\n",
    "       [4.39000000e+00, 3.76832000e+03],\n",
    "       [1.13300000e+01, 1.42453100e+04],\n",
    "       [9.88000000e+00, 1.54796000e+03],\n",
    "       [7.49100000e+01, 1.15433729e+06],\n",
    "       [1.01600000e+01, 2.21075600e+05],\n",
    "       [1.43800000e+01, 2.77751000e+03],\n",
    "       [7.60000000e+00, 6.36265000e+03],\n",
    "       [1.15100000e+01, 1.40526600e+04],\n",
    "       [9.83000000e+00, 6.19777800e+04],\n",
    "       [1.35600000e+01, 1.12027940e+05],\n",
    "       [6.29100000e+01, 1.37365337e+06],\n",
    "       [1.59000000e+01, 7.66850000e+03],\n",
    "       [2.58800000e+01, 4.67021540e+05],\n",
    "       [3.47200000e+01, 2.30052900e+05]])\n",
    "\n",
    "# Separating into just rna and protein to make flow simpler\n",
    "rna = rna_protein[:, 0]\n",
    "protein = rna_protein[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our data look like? \n",
    "\n",
    "sns.scatterplot(rna,protein)\n",
    "plt.xlabel(\"RNA molecules/cell\")\n",
    "plt.ylabel(\"Protein molecules/cell\")\n",
    "plt.title(\"Relationship between RNA and protein abundance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting a line to these data. Here you can use the formulas given to calculate the slope $b$ and y-intercept $a$, or rewrite the code block and use statsmodels or sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slope and intercept protein = a + b*rna \n",
    "rna_mean = np.mean(rna)\n",
    "protein_mean = np.mean(protein)\n",
    "\n",
    "# Add your own code to calculate a and b\n",
    "b = \n",
    "a = \n",
    "\n",
    "print(a,b)\n",
    "\n",
    "# Plot the line of best fit\n",
    "\n",
    "sns.lineplot(rna, a+ (b*rna), label=\"best fit\")\n",
    "sns.scatterplot(rna, protein)\n",
    "plt.xlabel(\"RNA molecules/cell\")\n",
    "plt.ylabel(\"Protein molecules/cell\")\n",
    "plt.title(\"Relationship between RNA and protein abundance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did we do a good job? \n",
    "\n",
    "A **residual plot** is a scatter plot of the residuals $(Y_i - \\hat{Y}_i)$ against the values of the explanatory variable (rna for our case). Linear regression assumes that the variance of y-values is the same across all x-values. So our residual plot _should_ look like dots within a thin horizontal band, vertically centered at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the Y hats (the values of protein predicted by our best-fitting line)\n",
    "protein_hats = a + b*rna\n",
    "\n",
    "# Calculate the residuals - add your own code!\n",
    "residuals = \n",
    "\n",
    "# Plot the residual plot\n",
    "sns.scatterplot(rna, residuals)\n",
    "plt.xlabel(\"RNA molecules/cell\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bad. The fit gets worse and worse as we have more RNA molecules/cell. Definitely we cannot assume that the variance of Y-values at each value of X is the same.\n",
    "\n",
    "What do we do instead?\n",
    "\n",
    "\n",
    "## Data tranformations\n",
    "\n",
    "In statistics, if the original data don't fit the assumptions of a particular statistical test, often times we try [transforming](http://www.biostathandbook.com/transformation.html) the data and if a transformation of our data fit the assumptions, we can proceed. We can try transforming our RNA/protein data to see if it will fit the linear regression assumptions.\n",
    "What transformation should we try here?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What transformation should we try? Try square root, logarithm, and multiplying times a scalar. Which is best?\n",
    "\n",
    "transformed_protein = np.log(protein)\n",
    "transformed_rna = np.log(rna)\n",
    "\n",
    "# Check our transformation using a scatterplot and show regression line\n",
    "\n",
    "b_transformed = (np.sum((transformed_rna - transformed_rna.mean())*(transformed_protein - transformed_protein.mean())))/(np.sum((transformed_rna - transformed_rna.mean())**2))\n",
    "a_transformed = transformed_protein.mean() - b_transformed*transformed_rna.mean()\n",
    "\n",
    "\n",
    "# Plot!\n",
    "sns.scatterplot(transformed_rna, transformed_protein)\n",
    "sns.lineplot(transformed_rna, a_transformed + b_transformed*transformed_rna)\n",
    "# Change labels!\n",
    "plt.xlabel(\"transformed RNA molecules/cell\")\n",
    "plt.ylabel(\"tranformed protein molecules/cell\")\n",
    "plt.title(\"Transformed scatterplot\");\n",
    "\n",
    "print(a_transformed,b_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residual plot after our transformation. Is it any better?\n",
    "\n",
    "transformed_protein_hats = a_transformed + b_transformed*transformed_rna\n",
    "transformed_residuals = transformed_protein - transformed_protein_hats\n",
    "\n",
    "sns.scatterplot(transformed_rna, transformed_residuals)\n",
    "plt.xlabel(\"transformed rna molecules/cell\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting assumptions is not the only way transformations help us. Let's say we had a relationship that was not so linear looking..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "x_values = np.arange(100)\n",
    "y1_values = 3*(x_values)**2\n",
    "y2_values = 5*np.float_power(2, x_values)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(x_values, y1_values)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Non-linear relationship. y=3x^2\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.scatterplot(np.log(x_values), np.log(y1_values))\n",
    "plt.xlabel(\"ln(X)\")\n",
    "plt.ylabel(\"ln(Y)\")\n",
    "plt.title(\"Transformed relationship. ln(y)=ln(3) + 2ln(x)\")\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.scatterplot(x_values, y2_values)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Non-linear relationship. y=5(2^x)\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(x_values, np.log(y2_values))\n",
    "plt.xlabel(\"ln(X)\")\n",
    "plt.ylabel(\"ln(Y)\")\n",
    "plt.title(\"Transformed relationship. ln(y)=ln(5) + ln(2)x\")\n",
    "\n",
    "# Adjust spacing between the subplots\n",
    "plt.subplots_adjust(bottom=None, right=1, top=None, wspace=0.4, hspace = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolating \n",
    "\n",
    "So now we have a decent linear relation. Does that mean that when we measure the RNA abundance of a new gene, that we can immediately know it's protein level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an RNA expression count for some new gene\n",
    "new_gene_rna = 10\n",
    "\n",
    "# Plot the new data point where we expect it to fall on our scatter\n",
    "transformed_new_gene = np.log(new_gene_rna)\n",
    "\n",
    "# Plot!\n",
    "\n",
    "sns.scatterplot(transformed_rna, transformed_protein)\n",
    "sns.lineplot(transformed_rna, a_transformed + b_transformed*transformed_rna)\n",
    "sns.scatterplot([transformed_new_gene], [a_transformed + b_transformed*transformed_new_gene], s=100, label = \"new gene\")\n",
    "\n",
    "plt.xlabel(\"Transformed RNA molecules/cell\")\n",
    "plt.ylabel(\"Transformed protein molecules/cell\")\n",
    "plt.title(\"Transformed scatterplot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we saw 0 RNA molecules? Or 3000? What does our model say about the protein levels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the value of new_gene_rna \n",
    "\n",
    "new_gene_rna = 0\n",
    "\n",
    "# Plot the new data point where we expect it to fall on our scatter\n",
    "transformed_new_gene = np.log(new_gene_rna)\n",
    "\n",
    "# Plot!\n",
    "sns.scatterplot(transformed_rna, transformed_protein)\n",
    "sns.lineplot(transformed_rna, a_transformed + b_transformed*transformed_rna)\n",
    "sns.scatterplot([transformed_new_gene], [a_transformed + b_transformed*transformed_new_gene], s=100, label = \"new gene\")\n",
    "\n",
    "plt.xlabel(\"Transformed RNA molecules/cell\")\n",
    "plt.ylabel(\"Transformed protein/cell\")\n",
    "plt.title(\"Transformed scatterplot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we see 0 expression, we still expect non-zero protein levels!\n",
    "\n",
    "**Warning** Extrapolating beyond the range of values we used to perform the regression can lead to impossible and unlikely results -- **Don't do it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Effects of error\n",
    "\n",
    "Let's give the researchers a break -- there was probably just a lot of measurement error in the observations. Maybe the relationship is real but the act of measuring these quantities has some inherant noise. \n",
    "\n",
    "While we may not be able to answer that question, what *would* error do to a real linear relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same x_values as before\n",
    "y_values = 3 - 4*x_values\n",
    "\n",
    "# To make things quicker, use sklearn to calculate linear regression values\n",
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "\n",
    "# Plot the original data\n",
    "plt.subplot(2, 2, 1)\n",
    "\n",
    "regr.fit(x_values.reshape(-1,1), y_values.reshape(-1,1))\n",
    "\n",
    "sns.regplot(x_values, y_values)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(f'No noise\\n y={regr.intercept_} + {regr.coef_[0]}x')\n",
    "\n",
    "# Add noise JUST to x values\n",
    "# Adjust the low, high values to adjust the noise range\n",
    "x_values_noise = x_values + np.random.uniform(low=-1000,high=1000, size=x_values.size)\n",
    "\n",
    "\n",
    "regr.fit(x_values_noise.reshape(-1,1), y_values.reshape(-1,1))\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(x_values_noise, y_values)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(f\"Noise added to X\\n y={regr.intercept_} + {regr.coef_[0]}x \")\n",
    "\n",
    "\n",
    "\n",
    "# Add noise to JUST the y values\n",
    "y_values_noise = y_values + np.random.uniform(low=-1000, high=1000, size = y_values.size)\n",
    "\n",
    "regr.fit(x_values.reshape(-1,1), y_values_noise.reshape(-1,1))\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.regplot(x_values, y_values_noise)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(f\"Noise added to Y\\n y={regr.intercept_} + {regr.coef_[0]}x \")\n",
    "\n",
    "\n",
    "# Add noise to both\n",
    "\n",
    "regr.fit(x_values_noise.reshape(-1,1), y_values_noise.reshape(-1,1))\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.regplot(x_values_noise, y_values_noise, fit_reg=True)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(f\"Noise added to both X and Y\\n y={regr.intercept_} + {regr.coef_[0]}x \")\n",
    "\n",
    "# Adjust spacing between the subplots\n",
    "plt.subplots_adjust(bottom=None, right=1, top=None, wspace=0.4, hspace = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## Regression towards the mean\n",
    "\n",
    "This concept is a little tricky, so we will demonstrate with our collected standing jump data. \n",
    "\n",
    "The overall idea is that if you measure something on an individual and then measure that something again at a later date, the natural variability in the individual will lessen the regression slope. From The Analysis of Biological Data (Whitlock):\n",
    "\n",
    "> Regression toward the mean results when two variables measured on a sample of individuals have a correlation less than one. Individuals that are far from the mean for one of the measurements will, on average, lie closer to the mean for the other measurements.\n",
    "\n",
    "Let's see if we observe this phenomenon with our jumping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "\n",
    "heights = np.array([64, 64, 67, 62, 62.5, 68, 73, 73, 63, 64, 69, 64, 60])\n",
    "\n",
    "jump1 = np.array([132, 74, 60, 49, 52, 46, 40, 90, 71, 73, 67, 30, 41])\n",
    "\n",
    "jump2 = np.array([130, 75, 60, 52, 50, 40.5, 43, 87, 68, 76, 65, 26, 50])\n",
    "\n",
    "avg_jump = np.mean([jump1, jump2], axis=0)\n",
    "\n",
    "# Let's put the data into a pandas dataframe to match what we did before\n",
    "jump_df = pd.DataFrame({\"height\": heights, \"jump1\": jump1, \"jump2\": jump2, \"avg_jump\": avg_jump})\n",
    "jump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the first jump results against the second jump\n",
    "\n",
    "print(f\"The mean of jump 1 results = {np.mean(jump1)} in \\n and the mean of jump 2 results = {np.mean(jump2)} in\")\n",
    "sns.scatterplot(jump1, jump2)\n",
    "plt.xlabel(\"Distance jump 1 (in)\")\n",
    "plt.ylabel(\"Distance jump 2 (in)\")\n",
    "plt.title(\"Regression towards the mean example\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think regression towards the mean is a problem here? Why or why not?\n",
    "\n",
    "\n",
    "Regression towards the mean can be a real problem for medical studies. For example, if we looked at the response of sick individuals to a treatment, people will appear to get better even though the treatment may have had no effect, just because the individuals well-being is tending back towards the average. Having a control group in your study prevents this misinterpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Fitting data to curves\n",
    "\n",
    "Today we saw many examples of how to find curves or functions that fit our data. However, perhaps the most important lesson is the one in the following code block. Curve fitting is a blanace between finding a function that approprately fits the data we have, and using a function that is general enough to reasonably help us make expectations about what new data might look like. Consider the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some fake x data\n",
    "x = np.arange(10)\n",
    "\n",
    "# Now choose y values that all come from the same normal distribution\n",
    "y = np.random.normal(0,0.2,x.size)\n",
    "\n",
    "# Draw a scatterplot of the data\n",
    "plt.figure(figsize=(15,9))\n",
    "plt.subplot(2,3,1)\n",
    "sns.scatterplot(x,y)\n",
    "plt.title(\"Original data\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "# Let's draw a curve that really nicely hits every point in our data\n",
    "interpolated_function_y = interp1d(x, y, kind='cubic')\n",
    "\n",
    "# Plot this function atop the scatter\n",
    "plt.subplot(2,3,2)\n",
    "sns.scatterplot(x,y)\n",
    "sns.lineplot(np.linspace(0,9,1000), interpolated_function_y(np.linspace(0,9,1000)))\n",
    "plt.title(\"Fitting every data point\")\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "# But really we just have data distributed along the vertical line y=0. So the actual relation is linear.\n",
    "plt.subplot(2,3,3)\n",
    "sns.scatterplot(x,y)\n",
    "sns.lineplot(x, np.zeros(x.size))\n",
    "plt.title(\"Linear relation\")\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "\n",
    "# Okay so obviously the interpolated curve fits better. But the real test comes when we get NEW data.\n",
    "# How well do our fits predict where new data might lie?\n",
    "\n",
    "# Make 3 new data points\n",
    "x_new = np.random.randint(9, size =3)\n",
    "y_new = np.random.normal(0,0.2,x_new.size)\n",
    "\n",
    "# Draw the updated scatterplot\n",
    "plt.subplot(2,3,4)\n",
    "sns.scatterplot(x,y, label = \"original\")\n",
    "sns.scatterplot(x_new,y_new, label = \"new\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "# Would these new points fall near our original highly complicated curve?\n",
    "plt.subplot(2,3,5)\n",
    "sns.scatterplot(x,y, label = \"original\")\n",
    "sns.lineplot(np.linspace(0,9,1000), interpolated_function_y(np.linspace(0,9,1000)))\n",
    "sns.scatterplot(x_new,y_new, label = \"new\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylim(-1,1)\n",
    "\n",
    "# What about the line?\n",
    "plt.subplot(2,3,6)\n",
    "sns.scatterplot(x,y, label = \"original\")\n",
    "sns.lineplot(x, np.zeros(x.size))\n",
    "sns.scatterplot(x_new,y_new, label = \"new\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylim(-1,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that when using a curve that fits every single datapoint perfectly, we generally have trouble when new datapoints come along. This happens because the curve has _overfit_ the data. Instead, using a mathematically simpler curve (here a line) results in more reasonable predictions when new datapoints are added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Other examples of nonlinear fitting - Quadratic equations\n",
    "\n",
    "We can use curve fitting to fit data to any curve -- including a quadratic curve: \n",
    "$$ y = a+bx+cx^2$$\n",
    "Let's take a look at how that works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's define x and y values\n",
    "x_quad = np.linspace(-100,100,100)\n",
    "y_quad = 5+2*x_quad+ 3*x_quad**2\n",
    "\n",
    "# add noise\n",
    "y_quad_noise = y_quad + 10000 * np.random.normal(size=x_quad.size)\n",
    "\n",
    "\n",
    "# define quadratic fit a + b*(x-c)**2\n",
    "def quad_func(x,a,b,c):\n",
    "    return a + b*x+c*x**2\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x_quad, y_quad)\n",
    "plt.title(\"Quadratic data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x_quad, y_quad_noise)\n",
    "plt.title(\"Noisy quadratic data\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "# Perform curve fitting\n",
    "popt_quad, pcov_quad = curve_fit(quad_func, x_quad, y_quad_noise)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x_quad, y_quad_noise)\n",
    "plt.plot(x_quad, quad_func(x_quad, *popt_quad), '-',\n",
    "         color =\"orange\",\n",
    "         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt_quad),\n",
    "         linewidth = 4)\n",
    "plt.title(\"Noisy data fit with quadratic function\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "\n",
    "\n",
    "plt.subplots_adjust(bottom=None, right=1, top=None, wspace=0.4, hspace = 0.4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we don't have to stop at quadratic functions. With curve fitting the possibilities are endless!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
